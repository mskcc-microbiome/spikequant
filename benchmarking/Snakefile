import os
import json
import yaml
import shutil

from pathlib import Path
from snakemake.utils import validate




# the str is needed as when running from github workflow.current_basedir is a Githubfile, not a string or a path so os.path objects
configfile: os.path.join(str(workflow.current_basedir), "../config/config.yaml")



onstart:
    with open("config_used.yaml", "w") as outfile:
        yaml.dump(config, outfile)

    if not os.path.exists("logs"):
        os.makedirs("logs")


localrules:
    all,
    ani_filter,
#    add_spikes_to_bin_dir,

mappers = ["minimap2-sr", "bwa-mem"]
quanttype = ["allseqs", "primaryseq"]
dbtypes = [f"{x}-{y}" for x in ["spike", "assembly", "bins", "mock"] for y in quanttype]

covermreports = expand('coverm/{sample}_and_{dbtype}_via_{mapper}_bins.coverage_mqc.tsv',
                       sample=config["sample"],
                       mapper=mappers,
                       dbtype=dbtypes)

despiked_reads = expand('{sample}_and_{dbtype}_via_{mapper}_nospike_R{readdir}.fastq.gz',
                        sample=config["sample"],
                        readdir=[1,2],
                        mapper=mappers,
                        dbtype=dbtypes)
ani_results = expand('{sample}_and_{dbtype}_ani_results.tsv',
                        sample=config["sample"],
                        mapper=mappers,
                        dbtype=dbtypes)




wildcard_constraints:
    dbtype="spike-allseqs|assembly-allseqs|bins-allseqs|mock-allseqs|spike-primaryseq|assembly-primaryseq|bins-primaryseq|mock-primaryseq",
    mapper="minimap2-sr|bwa-mem",




rule all:
    input:
        despiked_reads,
        ani_results,
        covermreports,
        "all_coverage.tsv"


rule add_spikes_to_bin_dir:
    input:
        bindir=config["bindir"],
        assembly=config["assembly"]
    output:
        bindir=temp(directory("{sample}_and_bins-allseqs")),
        assemblydir=temp(directory("{sample}_and_assembly-allseqs")),
        mockdir=temp(directory("{sample}_and_mock-allseqs")),
        spikedir=temp(directory("{sample}_and_spike-allseqs")),
        # sr="{sample}_and_spike-allseqs/Salinibacter_ruber.fa",
        # hh="{sample}_and_spike-allseqs/Haloarcula_hispanica.fa",
        # tr="{sample}_and_spike-allseqs/Trichoderma_reesei.fa",
#    container: "docker://ghcr.io/vdblab/bowtie2:2.5.0"
    shell:"""
    wget -N https://s3.amazonaws.com/zymo-files/BioPool/D6331.refseq.zip
    unzip D6331.refseq.zip
    mkdir {output}
    mv D6331.refseq/genomes/* {output.mockdir}

    cp {input.bindir}/* {output.bindir}/
    cp {input.assembly} {output.assemblydir}

    curl -o - ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/013/045/GCA_000013045.1_ASM1304v1/GCA_000013045.1_ASM1304v1_genomic.fna.gz | gunzip > {output.spikedir}/Salinibacter_ruber.fa
    # getting Trichoderma reesei  QM6a instead of  Trichoderma reesei ATCC 13631 .  Might be fine license-wise but not sure
    curl -o - ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/167/675/GCF_000167675.1_v2.0/GCF_000167675.1_v2.0_genomic.fna.gz | gunzip  > {output.spikedir}/Trichoderma_reesei.fa
    # getting CBA1121 instead of Haloarcula hispanica ATCC 33960  for the same reason.
    curl -o - ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/008/729/095/GCF_008729095.1_ASM872909v1/GCF_008729095.1_ASM872909v1_genomic.fna.gz | gunzip  > {output.spikedir}/Haloarcula_hispanica.fa

    cp {output.spikedir}/*.fa {output.bindir}
    cp {output.spikedir}/*.fa {output.assemblydir}
    cp {output.spikedir}/*.fa {output.mockdir}
    """

rule split_spikes_fasta_primary_other:
    """ for each reference, split it into two files: first sequences and all subsequent seqs
    """
    input:
        # sr="{sample}_and_spike-allseqs/Salinibacter_ruber.fa",
        # hh="{sample}_and_spike-allseqs/Haloarcula_hispanica.fa",
        # tr="{sample}_and_spike-allseqs/Trichoderma_reesei.fa",
        indir="{sample}_and_spike-allseqs",
    output:
        outdir=directory("{sample}_splitspikes"),
        # psr="splitspike/Salinibacter_ruber_primary.fa",
        # phh="splitspike/Haloarcula_hispanica_primary.fa",
        # ptr="splitspike/Trichoderma_reesei_primary.fa",
        # osr="splitspike/Salinibacter_ruber_other.fa",
        # ohh="splitspike/Haloarcula_hispanica_other.fa",
        # otr="splitspike/Trichoderma_reesei_other.fa",
#        trigger=touch("{sample}_split_spikes.done"),
    run:
        import os
        os.makedirs(output.outdir, exist_ok=True)
        refs = ["Salinibacter_ruber", "Haloarcula_hispanica", "Trichoderma_reesei"]
        sets = {
            "sr": [
                os.path.join(input.indir, refs[0] + ".fa"),
                os.path.join(output.outdir, refs[0] + "_primary.fa"),
                os.path.join(output.outdir, refs[0] + "_other.fa")
            ],
            "hh": [
                os.path.join(input.indir, refs[1] + ".fa"),
                os.path.join(output.outdir, refs[1] + "_primary.fa"),
                os.path.join(output.outdir, refs[1] + "_other.fa") ],
            "tr": [
                os.path.join(input.indir, refs[2] + ".fa"),
                os.path.join(output.outdir, refs[2] + "_primary.fa"),
                os.path.join(output.outdir, refs[2] + "_other.fa")]
            }
        for k,v in sets.items():
            with open(v[0], "r") as inf, open(v[1], "w") as outp, open(v[2], "w") as outo:
                primary = True
                for i, line in enumerate(inf):
                    if i != 0 and line.startswith(">"):
                        primary = False
                    if primary:
                        outp.write(line)
                    else:
                        outo.write(line)


rule add_split_spikes_to_bin_dir:
    input:
        indir="{sample}_splitspikes",
        bindir="{sample}_and_bins-allseqs",
        assemblydir="{sample}_and_assembly-allseqs",
        mockdir="{sample}_and_mock-allseqs",
    output:
        pbindir=temp(directory("{sample}_and_bins-primaryseq")),
        passemblydir=temp(directory("{sample}_and_assembly-primaryseq")),
        pmockdir=temp(directory("{sample}_and_mock-primaryseq")),
        pspikedir=temp(directory("{sample}_and_spike-primaryseq")),
    container: "docker://ghcr.io/vdblab/bowtie2:2.5.0"
    shell:"""
    mkdir -p {output}
    cp {input.mockdir}/* {output.pmockdir}/
    cp {input.bindir}/* {output.pbindir}/
    cp {input.assemblydir}/* {output.passemblydir}/

    cp {input.indir}/*.fa {output.pbindir}
    cp {input.indir}/*.fa {output.passemblydir}
    cp {input.indir}/*.fa {output.pmockdir}
    cp {input.indir}/*.fa {output.pspikedir}

    # since we had already copied the non-split reference genomes,
    # we need to remove them from pbindir, passemblydir, and pmockdir
    # they aren't in pspikedir
    for d in {output.pbindir} {output.passemblydir} {output.pmockdir}
    do
    rm ${{d}}/Salinibacter_ruber.fa ${{d}}/Haloarcula_hispanica.fa ${{d}}/Trichoderma_reesei.fa
    done
    """

rule fastani:
    input:
        bindir="{sample}_and_{dbtype}",
#        trigger="{sample}_split_spikes.done",
    output:
        csv="{sample}_and_{dbtype}_ani_results.tsv",
        manifest="{sample}_and_{dbtype}_manifest.txt"
    container:"docker://ghcr.io/vdblab/fastani:1.34"
    threads: 32
    shell: """
    ls {input.bindir}/*fa > {output.manifest}
    fastANI --ql {output.manifest} --rl {output.manifest} --minFraction 0 --threads {threads} -o {output.csv}
    """

rule ani_filter:
    """ 95 based on https://www.nature.com/articles/s41467-018-07641-9
    """
    input:
        csv="{sample}_and_{dbtype}_ani_results.tsv",
        bindir="{sample}_and_{dbtype}_bins_with_spikes_raw",
        manifest="{sample}_and_{dbtype}_manifest.txt",
    output:
        pairs_for_vis="{sample}_and_{dbtype}_close_ani.tsv",
        bindir=temp(directory("{sample}_and_{dbtype}_bins_with_spikes")),
    params:
        ani_thresh = 95
    run:
        import os
        import sys
        import shutil

        contigs_to_ignore = []
        # could use pandas, but this is pretty straightforward
        with open(input.csv, "r") as inf, open(output.pairs_for_vis,  "w") as outf:
            for line in inf:
                (query, reference, ANI, count_mappings, total_frags) = line.strip().split()
                if query == reference:
                    continue
                # we generate vis for all comparisons exceeding 80%, just in case there are
                # any bin-bin comparisons of interest
                if float(ANI) > 80:
                    outf.write(line)
                # ... but only exclude contigs if they exceed the specified
                # threshold species-level
                if float(ANI) < params.ani_thresh:
                    continue
                if "Haloarcula_hispanica" in line or "Salinibacter_ruber" in line or "Trichoderma_reesei" in line:
                    # determine if the bin resembing the cannonical reference is the ani query or the reference
                    if "Haloarcula_hispanica" in query or "Salinibacter_ruber" in query or "Trichoderma_reesei" in query:
                        bin_index  = 1
                    else:
                        bin_index  = 0
                    contigs_to_ignore.append([query, reference][bin_index])
        os.makedirs(output.bindir)
        with open(input.manifest, "r") as inf:
            for line in inf:
                if line.strip() not in contigs_to_ignore:
                    dest =  os.path.join(output.bindir, os.path.basename(line.strip()))
                    if not os.path.exists(dest):
                        shutil.copyfile(line.strip(), dest)
                else:
                    print(f"Excluding {line.strip()}")
        print("done")



rule ani_vis:
    """ visualize the ones we exclude for being too close to the spikes,
    and any other with close ANI (>80%)
    """
    input:
        pairs_for_vis="{sample}_and_{dbtype}_close_ani.tsv",
        bindir="{sample}_and_{dbtype}_bins_with_spikes_raw",
    output:
        figdir=directory("{sample}_and_{dbtype}_bin_ref_homology_figures"),
    container:"docker://ghcr.io/vdblab/fastani:1.34"
    params:
        ani_thresh=.9
    threads: 16
    shell: """
    mkdir {output.figdir}
    cat {input.pairs_for_vis} | cut -f 1,2,3 | while read query ref ANI
    do
        echo "Visualizing  $query vs  $ref (ANI of $ANI)"
        outbase={output.figdir}/$(basename $query)_$(basename $ref)_${{ANI}}
        fastANI -q $query -r $ref --minFraction 0 --threads {threads} --visualize -o ${{outbase}}.out

        Rscript /FastANI/scripts/visualize.R $query $ref  ${{outbase}}.out.visual
    done

    """


rule coverm:
    """ This calculates bin coverage.  The bin stats file is used as the
    trigger, but we actually want the directory of the bins

    specifying inputs with --coupled and -1 -2 seem to give equivalent results.
    Note that the output only refers tothe forward file as the sample name.
    minimap2 is supposedly faster and more accurate than BWA, so we use that
    as the mapper.
    we return all the available methods as of the time of writing this rule
    except for coverage_histogram which has to run separately
    ("Cannot specify the coverage_histogram method with any other coverage methods")

    --min-covered-fraction is required to be set to 0 for certain cov metrics
     --genome-fasta-extension is fa since thats how metawrap outputs it
    """
    input:
        R1=config["R1"],
        R2=config["R2"],
        bindir="{sample}_and_{dbtype}",
#        trigger="{sample}_split_spikes.done",
    output:
        mqc='coverm/{sample}_and_{dbtype}_via_{mapper}_bins.coverage_mqc.tsv',
        bam=os.path.join('coverm/{sample}_and_{dbtype}_via_{mapper}_bams/', 'coverm-genome.' + os.path.basename(config["R1"][0]) + ".bam"),
    params:
        fastq_string = lambda wc, input: " ".join([config["R1"][x] + " " + config["R2"][x] for x in range(0, len(config["R1"]))])
    resources:
        runtime=4*60
    container:
        config["docker_coverm"]
    threads: 32
    shell:
        """
        coverm genome --genome-fasta-directory {input.bindir} \
          --coupled {params.fastq_string} \
          --mapper {wildcards.mapper} \
          --methods mean relative_abundance trimmed_mean \
            covered_bases variance length count reads_per_base rpkm tpm \
          --output-file {output.mqc}.tmp --threads {threads} \
          --bam-file-cache-directory $(dirname  {output.bam}) \
          --min-covered-fraction 0 \
          --genome-fasta-extension fa
        ls $(dirname  {output.bam})
        # add in the Multiqc header info
        echo -e "# plot_type: 'table'\n# section_name: 'Bin Coverage Statistics'" > {output.mqc}
        cat {output.mqc}.tmp >> {output.mqc}
        rm {output.mqc}.tmp
        """


rule get_nonspike_reads:
    input:
        bam=rules.coverm.output.bam,
    output:
        bed="{sample}_and_{dbtype}_via_{mapper}_spike_regions.bed",
        nonspike=temp("{sample}_and_{dbtype}_via_{mapper}_nonspike.bam"),
        R1="{sample}_and_{dbtype}_via_{mapper}_nospike_R1.fastq.gz",
        R2="{sample}_and_{dbtype}_via_{mapper}_nospike_R2.fastq.gz",
        spikeR1="{sample}_and_{dbtype}_via_{mapper}_spike_R1.fastq.gz",
        spikeR2="{sample}_and_{dbtype}_via_{mapper}_spike_R2.fastq.gz",
    threads: 4
    params:
        R1tmp=lambda wc, output: output.R1.replace(".gz", ""),
        R2tmp=lambda wc, output: output.R2.replace(".gz", ""),
        R1tmpspike=lambda wc, output: output.spikeR1.replace(".gz", ""),
        R2tmpspike=lambda wc, output: output.spikeR2.replace(".gz", ""),
    container: "docker://ghcr.io/vdblab/bowtie2:2.5.0"
    shell:"""
    set -eoux pipefail
    # get header, turn into a BED file
    samtools view -H  {input.bam} | grep "Salinibacter\|Trichoderma\|Haloarcula" | cut -f 2,3 | sed "s|SN:||g" | sed "s|LN:|1\t|g" | sort -k1,1 -k2,2n > {output.bed}

    # inspired by https://www.biostars.org/p/473204/
    samtools view -L {output.bed} -U unsorted_{output.nonspike} -o tmp_unsorted_spikes_{wildcards.sample}_and_{wildcards.dbtype}_via_{wildcards.mapper}.bam -@ {threads} {input.bam}
    # make sure you NAME-sort before running samtools fastq https://www.biostars.org/p/454942/
    samtools sort -n -o {output.nonspike} unsorted_{output.nonspike}
    samtools sort -n -o tmp_sorted_spikes_{wildcards.sample}_and_{wildcards.dbtype}_via_{wildcards.mapper}.bam tmp_unsorted_spikes_{wildcards.sample}_and_{wildcards.dbtype}_via_{wildcards.mapper}.bam

    # samtools can ouput copressed fastqs but it isn't multithreaded as fast as pigz
    samtools fastq -1 {params.R1tmp} -2 {params.R2tmp} -0 /dev/null -s /dev/null -n -@ {threads} {output.nonspike}
    samtools fastq -1 {params.R1tmpspike} -2 {params.R2tmpspike} -0 /dev/null -s /dev/null -n -@ {threads} tmp_sorted_spikes_{wildcards.sample}_and_{wildcards.dbtype}_via_{wildcards.mapper}.bam
    samtools flagstat {output.nonspike}

    rm unsorted_{output.nonspike} tmp_unsorted_spikes_{wildcards.sample}_and_{wildcards.dbtype}_via_{wildcards.mapper}.bam tmp_sorted_spikes_{wildcards.sample}_and_{wildcards.dbtype}_via_{wildcards.mapper}.bam
    pigz {params.R1tmp}
    pigz {params.R2tmp}
    pigz {params.R1tmpspike}
    pigz {params.R2tmpspike}
    """

rule tabulate:
    input:
        covermreports
    output:
        "all_coverage.tsv"
    shell: """
    echo -e "sample\tGenome\tMean\tRelative_Abundance_Perc\tTrimmed_Mean\tCovered_Bases\tVariance\tLength\tRead_Count\tReads_per_base\tRPKM\rTPM" > {output[0]}
    for rep in {input}
    do
    base=$( basename $rep  | sed "s|_bins.coverage_mqc.tsv||g")
    tail -n+4 $rep | sed "s|^|${{base}}\t|g" >> {output[0]}
    done
    """
