import os
import json
import yaml
import shutil
import glob
import pandas as pd
from pathlib import Path
from snakemake.utils import validate

# the str is needed as when running from github workflow.current_basedir is a Githubfile, not a string or a path so os.path objects
configfile: os.path.join(str(workflow.current_basedir), "../config/config.yaml")


#include: "common.smk"

# ToDo change this to schema validation
assert "spiketable" in config.keys(), "spiketable is a required config argument!"

#mapper = "minimap2-sr"
#if "mapper" in config.keys():
if config["mapper"] == "bowtie2":
    raise ValueError("Ideal alignment parameters for bowtie2 have not been identified currently")

assert config["mapper"] in ["minimap2-sr", "bwa-mem"], "aligner must be either minimap2-sr , or bwa-mem"
#    mapper = config["mapper"]

offtarget = None
if "offtarget" in config.keys():
    offtarget = config["offtarget"]







SPIKEDF = pd.read_csv(config["spiketable"], sep=",", names=['name', "path","bedpath"], header=None)
beds = {k:v for k,v in  zip(SPIKEDF.name.tolist(), SPIKEDF.bedpath.tolist())}

for i in SPIKEDF.bedpath.tolist():
    if not os.path.exists(i):
        raise ValueError(f"{i} not found")

for i in SPIKEDF.path.tolist():
    if not os.path.exists(i):
        raise ValueError(f"{i} not found")

#df["sample"] = manif_base
#df = df.set_index("sample")


envvars:
    "TMPDIR",

onstart:
    with open("config_used.yaml", "w") as outfile:
        yaml.dump(config, outfile)

    if not os.path.exists("logs"):
        os.makedirs("logs")


localrules:
    all,
    ani_filter







rule all:
    input:
        f'{config["sample"]}.spike_reads.tsv',
        f'{config["sample"]}_nospike_R1.fastq.gz',
        f'{config["sample"]}_nospike_R2.fastq.gz',
        f'{config["sample"]}_ani_results.tsv',
        f'{config["sample"]}.stats',

def get_spike_abs_paths(wildcards):
    return SPIKEDF.path.tolist()

rule copy_spikes_and_offtarget:
    input:
        spikes = get_spike_abs_paths,
    params:
        # this has to be a param to support "None"
        offtarget=config["offtarget"],
    output:
        bindir=temp(directory("{sample}_with_spikes_raw"))
    run:
        os.makedirs(output.bindir)
        for spike in input.spikes:
            shutil.copyfile(spike, os.path.join(output.bindir, os.path.basename(spike)))
        if params.offtarget.lower() == "none":
            pass
        elif os.path.isdir(params.offtarget):
            print("offtartget is directory")
            fastas = glob.glob(params.offtarget + "*fa") + glob.glob(params.offtarget + "*fasta") + glob.glob(params.offtarget + "*fna") + \
                glob.glob(params.offtarget + "*fa.gz") + glob.glob(params.offtarget + "*fasta.gz") + glob.glob(params.offtarget + "*fna.gz")
            for fasta in fastas:
                shutil.copy(fasta, os.path.join(output.bindir, os.path.basename(fasta)))
        else:
            assert os.path.exists(params.offtarget), f"{config['offtarget']} is not a file or a directory"
            shutil.copy(params.offtarget, os.path.join(output.bindir, os.path.basename(params.offtarget)))


rule fastani:
    input:
        bindir="{sample}_with_spikes_raw"
    output:
        csv="{sample}_ani_results.tsv",
        manifest="{sample}_manifest.txt"
    container:"docker://ghcr.io/vdblab/fastani:1.34"
    threads: 32
    shell: """
    ls {input.bindir}/*fa > {output.manifest}
    fastANI --ql {output.manifest} --rl {output.manifest} --minFraction 0 --threads {threads} -o {output.csv}
    """

rule ani_filter:
    """ 95 based on https://www.nature.com/articles/s41467-018-07641-9
    """
    input:
        spiketable=config["spiketable"],
        csv="{sample}_ani_results.tsv",
        bindir="{sample}_with_spikes_raw",
        manifest="{sample}_manifest.txt",
    output:
        pairs_for_vis="{sample}_close_ani.tsv",
        ref="{sample}_reference.fa",
    params:
        ani_thresh = 95
    run:
        import os
        import sys
        import shutil
        # have to do this here, rather than the main workflow and passing params.
        # otherwise, passing the entries as params kept being considered a param change
        # which would trigger rerunning.
        spike_names = pd.read_csv(config["spiketable"], sep=",", names=['name', "path","bedpath"], header=None).name.tolist()

        contigs_to_ignore = []
        # could use pandas, but this is pretty straightforward
        with open(input.csv, "r") as inf, open(output.pairs_for_vis,  "w") as outf:
            for line in inf:
                (query, reference, ANI, count_mappings, total_frags) = line.strip().split()
                if query == reference:
                    continue
                # we generate vis for all comparisons exceeding 80%, just in case there are
                # any bin-bin comparisons of interest
                if float(ANI) > 80.0:
                    outf.write(line)
                # ... but only exclude contigs if they exceed the specified
                # threshold species-level
                if float(ANI) < params.ani_thresh:
                    continue
                line_is_spike = False
                bin_index = None
                for spike_name in spike_names:
                    if spike_name in line:
                        line_is_spike = True
                        if spike_name in query:
                            bin_index = 1
                        else:
                            bin_index = 0
                        contigs_to_ignore.append([query, reference][bin_index])
#        os.makedirs(output.bindir)
        with open(input.manifest, "r") as inf, open(output.ref, "w") as outf:
            for line in inf:
                if line.strip() not in contigs_to_ignore:
                    with open(line.strip(), "r") as fasta:
                        for faline in fasta:
                            if faline.startswith(">"):
                                header = faline.replace(">", "")
                                id = header.split()[0]
                                genome_name = os.path.basename(line.strip().replace(".fa", "---"))
                                faline = f">{genome_name}{id}\n"
                            outf.write(faline)
                    #dest =  os.path.join(output.bindir, os.path.basename(line.strip()))
                    #if not os.path.exists(dest):
                    #    shutil.copyfile(line.strip(), dest)
                else:
                    print(f"Excluding {line.strip()}")
        print("done")



rule ani_vis:
    """ visualize the ones we exclude for being too close to the spikes,
    and any other with close ANI (>80%)
    """
    input:
        pairs_for_vis="{sample}_close_ani.tsv",
#        bindir="{sample}_with_spikes_raw",
    output:
        figdir=directory("{sample}_bin_ref_homology_figures"),
    container:"docker://ghcr.io/vdblab/fastani:1.34"
    params:
        ani_thresh=.9
    threads: 32
    shell: """
    mkdir {output.figdir}
    cat {input.pairs_for_vis} | cut -f 1,2,3 | while read query ref ANI
    do
        echo "Visualizing  $query vs  $ref (ANI of $ANI)"
        outbase={output.figdir}/$(basename $query)_$(basename $ref)_${{ANI}}
        fastANI -q $query -r $ref --minFraction 0 --threads {threads} --visualize -o ${{outbase}}.out

        Rscript /FastANI/scripts/visualize.R $query $ref  ${{outbase}}.out.visual
    done

    """

# rule make_spike_reference:
#     input:
#         bindir="{sample}_clean",
#     output:
#     shell:"""
#     for i in  {input.bindir}/*.fa
#     do
#     bname=$(basename $i)
#     bname=$(echo $bname | sed "s|.fa||g")
#     echo $bname
#     cat $i | sed "s|>|>${{bname}}---|g" >> {output.ref}
#     done
#     """

if config["mapper"] == "bwa-mem":
    rule bwa_mem_index:
        input:
            "{sample}_reference.fa",
        output:
            "{sample}_reference.fa.bwt",
            "{sample}_reference.fa.amb",
            "{sample}_reference.fa.ann",
            "{sample}_reference.fa.sa",
        log:
            "logs/bwa-mem2_index/{sample}.log",
        container:
            "docker://staphb/bwa:0.7.18"
        shell: "bwa index {input}"


    rule bwa_mem:
        input:
            reads=[config["R1"], config["R2"]],
            # Index can be a list of (all) files created by bwa, or one of them
            dbbase="{sample}_reference.fa",
            idx=multiext("{sample}_reference.fa", ".amb", ".ann", ".sa", ".bwt"),
        output:
            bam=os.path.join('{sample}_bams/{sample}.bam'),
    #        "mapped/{sample}.bam",
        log:
            "logs/bwa_mem/{sample}.log",
        params:
    #        extra=r"-R '@RG\tID:{sample}\tSM:{sample}'",
            sort="samtools",  # Can be 'none', 'samtools', or 'picard'.
            sort_order="coordinate",  # Can be 'coordinate' (default) or 'queryname'.
    #        sort_extra="",  # Extra args for samtools/picard sorts.
        threads: 32
        container:
            "docker://staphb/bwa:0.7.18"
        shell:"""
        bwa mem -t {threads} {input.dbbase} {input.reads} > {output.bam}
        """

elif config["mapper"] == "bowtie2":
    rule bowtie2:
        input:
            reads=[config["R1"], config["R2"]],
            dbbase="{sample}_reference.fa",
        output:
            bam=os.path.join('{sample}_bams/{sample}.bam'),
        container:"docker://staphb/bowtie2:2.5.3"
        threads: 32
        shell:"""
        bowtie2-build --threads {threads} {input.dbbase} {input.dbbase}
        bowtie2 --threads {threads} -x {input.dbbase} -1 {input.reads[0]} -2 {input.reads[0]}  --very-sensitive --no-discordant > {output.bam}
        """
else:
    assert config["mapper"] == "minimap2-sr", f'{config["mapper"]} unsupported'
    rule minimap2sr:
        input:
            reads=[config["R1"], config["R2"]],
            dbbase="{sample}_reference.fa",
        output:
            bam=os.path.join('{sample}_bams/{sample}.bam'),
        log:
            "logs/minimap2/{sample}.log",
        threads: 32
        container:
            "docker://staphb/minimap2:2.28"
        shell:"""
        minimap2 -d {input.dbbase}.mmi {input.dbbase}
        minimap2 -ax sr -t {threads}  {input.dbbase}.mmi {input.reads} > {output.bam}
        """


rule index_and_clean:
    input:
        sam=os.path.join('{sample}_bams/{sample}.bam'),
    output:
        bam="{sample}_bams/{sample}_clean.bam",
        bai="{sample}_bams/{sample}_clean.bam.bai",
    container: "docker://ghcr.io/vdblab/bowtie2:2.5.0"
    threads: 8
    shell: """
    # clean up bed names by extracting the appropriate header from alignment header,
    # removing the junk, and using those names to reheader the bam
    # that bam gets indexed and used for coverage calculation
    samtools sort -@{threads} -o {input.sam}.bam {input.sam}
    samtools view -H {input.sam}.bam |\
    sed -e 's/SN:.*---/SN:/' |  samtools reheader - {input.sam}.bam > {output.bam}.tmp
    # 3852 negated should keep all aligned, properly paired, primary alignments
    # rname == rnext should keep only concordant pairs
    # "(rlen(length(seq)) >= .97" should retain only alignments where there was greater than 97% overlap
    #  -e '[NM]/rlen <= .03' should retain only alignments where there was greater than 97% idenity
    samtools view -bh -F 3852   {output.bam}.tmp | samtools view -bh -e "rname == rnext" | samtools view -bh -e "((rlen/length(seq)) >= .97) && ([NM]/rlen <= .03)" > {output.bam}
    # now, we need to filter to mimic what coverm was doing: retaining only properly aligned pairs,
    samtools index  {output.bam}
    """


def get_bed_from_wc(wildcards):
    target_bed = beds[wildcards.bed]
    return target_bed


rule run_bed_based_coverage_calculations:
    input:
        bam="{sample}_bams/{sample}_clean.bam",
        bai="{sample}_bams/{sample}_clean.bam.bai",
        bed=get_bed_from_wc,
    output:
        tsv="{sample}_bed{bed}.tsv",
        cov="{sample}_bed{bed}.cov",
    container: "docker://ghcr.io/vdblab/bowtie2:2.5.0"
    shell: """
    # get total bed length
    bedlen=$(awk -F'\t' 'BEGIN{{SUM=0}}{{SUM+=$3-$2 }}END{{print SUM}}' {input.bed})
    # subset bam to regions in the bed file.  The -f argument is much more
    # usable here rather than in multicov, because it is relative to the bam here.
    # we want to filter based on how much of the read overlaps the region, rather than
    # the other way around; this is because the regions have variable length, and this
    # argument is a fraction of the length not an integer of overlapping bases
    bedtools intersect -a {input.bam}  -b {input.bed} -f .5  > {output.cov}.tmp.bam

    samtools index {output.cov}.tmp.bam

    bedtools multicov -bams {output.cov}.tmp.bam  -bed {input.bed}   > {output.cov}

    reads_in_regions=$(awk '{{s+=$4}} END {{print s}}' {output.cov})
    echo -e "{wildcards.bed}\t${{bedlen}}\t${{reads_in_regions}}" > {output.tsv}

    rm  {output.cov}.tmp.*
    """


rule agg:
    input:
        tsv = expand("{{sample}}_bed{bed}.tsv", bed=beds.keys())
    output:
        "{sample}.spike_reads.tsv",
    shell: """
    cat {input} > {output}
    """


rule get_nonspike_reads:
    input:
        bam=os.path.join('{sample}_bams/{sample}.bam'),
    output:
        bed="{sample}_spike_regions.bed",
        nonspike=temp("{sample}_nonspike.bam"),
        R1="{sample}_nospike_R1.fastq.gz",
        R2="{sample}_nospike_R2.fastq.gz",
        spikeR1="{sample}_spike_R1.fastq.gz",
        spikeR2="{sample}_spike_R2.fastq.gz",
    threads: 4
    params:
        R1tmp=lambda wc, output: output.R1.replace(".gz", ""),
        R2tmp=lambda wc, output: output.R2.replace(".gz", ""),
        R1tmpspike=lambda wc, output: output.spikeR1.replace(".gz", ""),
        R2tmpspike=lambda wc, output: output.spikeR2.replace(".gz", ""),
    container: "docker://ghcr.io/vdblab/bowtie2:2.5.0"
    shell:"""
    set -eoux pipefail
    # get header, turn into a BED file
    samtools view -H  {input.bam} | grep "Salinibacter\|Trichoderma\|Haloarcula" | cut -f 2,3 | sed "s|SN:||g" | sed "s|LN:|1\t|g" | sort -k1,1 -k2,2n > {output.bed}

    # inspired by https://www.biostars.org/p/473204/
    samtools view -L {output.bed} -U unsorted_{output.nonspike} -o tmp_unsorted_spikes_{wildcards.sample}.bam -@ {threads} {input.bam}
    # make sure you NAME-sort before running samtools fastq https://www.biostars.org/p/454942/
    samtools sort -n -o {output.nonspike} unsorted_{output.nonspike}
    samtools sort -n -o tmp_sorted_spikes_{wildcards.sample}.bam tmp_unsorted_spikes_{wildcards.sample}.bam

    # samtools can ouput copressed fastqs but it isn't multithreaded as fast as pigz
    samtools fastq -1 {params.R1tmp} -2 {params.R2tmp} -0 /dev/null -s /dev/null -n -@ {threads} {output.nonspike}
    samtools fastq -1 {params.R1tmpspike} -2 {params.R2tmpspike} -0 /dev/null -s /dev/null -n -@ {threads} tmp_sorted_spikes_{wildcards.sample}.bam
    samtools flagstat {output.nonspike}

    rm unsorted_{output.nonspike} tmp_unsorted_spikes_{wildcards.sample}.bam tmp_sorted_spikes_{wildcards.sample}.bam
    pigz {params.R1tmp}
    pigz {params.R2tmp}
    pigz {params.R1tmpspike}
    pigz {params.R2tmpspike}
    """

rule get_despike_stats:
    input:
        R1="{sample}_nospike_R1.fastq.gz",
        R2="{sample}_nospike_R2.fastq.gz",
        spikeR1="{sample}_spike_R1.fastq.gz",
        spikeR2="{sample}_spike_R2.fastq.gz",
    output:
        stats="{sample}.stats",
    threads: 4
    container: "docker://ghcr.io/vdblab/seqkit:2.3.1"
    shell:"""
    seqkit stats {input} > {output.stats}
    """
