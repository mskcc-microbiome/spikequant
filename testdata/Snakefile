import os
import json
import yaml
import shutil

from pathlib import Path

# the str is needed as when running from github workflow.current_basedir is a Githubfile, not a string or a path so os.path objects
configfile: os.path.join(str(workflow.current_basedir), "config.yaml")

# this formats the fractions to strings so we don't have annoying outputs with sci notation or bad 0 padding
total_spike_fractions =  [f"{frac:.5f}" for frac in config["total_spike_fractions"]]
even_coverages =  [f"{frac:.5f}" for frac in config["even_coverages"]]

total_depths = config["total_depths"]
reps = config["reps"]

spiked_reads = expand('{rep}_depth{depth}_spike{frac}_R{readdir}.fastq.gz',
                      rep=reps,
                      depth=total_depths,
                      frac=total_spike_fractions,
                      readdir=[1,2])
spike_stats = expand("{rep}_depth{depth}_spike{frac}.statsfastq",
                     rep=reps,
                     depth=total_depths,
                     frac=total_spike_fractions,
                     )
evenspiked_reads = expand('{rep}_evencoverage{frac}_R{readdir}.fastq.gz',
                        rep=reps,
                        frac=even_coverages,
                        readdir=[1,2])
evenspike_stats = expand("{rep}_evencoverage{frac}.statsfastq",
                         rep=reps,
                         frac=even_coverages,
                         )

assemblies = expand("spades_{rep}_depth{depth}_spike{frac}.assembly.fasta",
                   rep=reps,
                   depth=total_depths,
                   frac=total_spike_fractions,
)
evenassemblies = expand("spades_{rep}_evencoverage{frac}.assembly.fasta",
                        rep=reps,
                        frac=even_coverages,
)

BINNING_TOOLS = ["concoct", "metabat2", "maxbin2"]
binstats = expand(
    "metawrap/rawbinning_{rep}_depth{depth}_spike{frac}/{tool}/{tool}_bins/{tool}.done",
                   rep=reps,
                   depth=total_depths,
                   frac=total_spike_fractions,
    tool=BINNING_TOOLS,
)
evenbinstats = expand(
    "metawrap/rawbinning_{rep}_evencoverage{fcov}/{tool}/{tool}_bins/{tool}.done",
    rep=reps,
    fcov=even_coverages,
    tool=BINNING_TOOLS,
)

refined_binstats_each = expand(
    "metawrap/refined_binning_{rep}_depth{depth}_spike{frac}/{tool}_bins.stats",
                   rep=reps,
                   depth=total_depths,
                   frac=total_spike_fractions,
    tool=BINNING_TOOLS,
)
refined_stats = expand(
    f'metawrap/refined_binning_{{rep}}_depth{{depth}}_spike{{frac}}/metawrap_{config["metawrap_compl_thresh"]}_{config["metawrap_contam_thresh"]}_bins.stats',
                       rep=reps,
                   depth=total_depths,
                   frac=total_spike_fractions,
)
refined_stats_even = expand(
    f'metawrap/refined_binning_{{rep}}_evencoverage{{fcov}}/metawrap_{config["metawrap_compl_thresh"]}_{config["metawrap_contam_thresh"]}_bins.stats',
    rep=reps,
    fcov=even_coverages,
)


localrules:
    all,
    make_input_table,

all_results = [
    spiked_reads,
    spike_stats,
    evenspiked_reads,
    evenspike_stats,
    evenassemblies,
    assemblies,
    binstats,
    evenbinstats,
    refined_stats_even,
    # "clean.done"
]

# only bother attempting to refine the bins if we have at least 10m reads
if min(total_depths) >= 10000000:
    all_results.extend([
        refined_stats,
        refined_binstats_each,
    ])

rule all:
    input:
        all_results


rule get_references:
    output:
        mockdir=directory("mock"),
        spikedir=directory("spikes"),
    params:
        exclude_t_reesei=config["exclude_t_reesei"],
    shell:"""
    set -eoux pipefail
    # wget complains if this is present
    if [ -f D6331.refseq.zip ]
    then
    echo "refs present"
    else
        wget -N https://s3.amazonaws.com/zymo-files/BioPool/D6331.refseq.zip
        unzip D6331.refseq.zip
    fi
    # snakemake doesn't create outputs labeled as directories
    mkdir -p {output.mockdir} {output.spikedir} tmp_spike

    curl -o - ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/013/045/GCA_000013045.1_ASM1304v1/GCA_000013045.1_ASM1304v1_genomic.fna.gz | gunzip > tmp_spike/Salinibacter_ruber.fa
    # getting Trichoderma reesei  QM6a instead of  Trichoderma reesei ATCC 13631 .  Might be fine license-wise but not sure
    if [ "{params.exclude_t_reesei}" != "true" ]
    then
       curl -o - ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/167/675/GCF_000167675.1_v2.0/GCF_000167675.1_v2.0_genomic.fna.gz | gunzip  > tmp_spike/Trichoderma_reesei.fa
    else
       echo "excluding T reesei"
    fi
    # getting CBA1121 instead of Haloarcula hispanica ATCC 33960  for the same reason.
    curl -o - ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/008/729/095/GCF_008729095.1_ASM872909v1/GCF_008729095.1_ASM872909v1_genomic.fna.gz | gunzip  > tmp_spike/Haloarcula_hispanica.fa

    for fasta in $(ls tmp_spike/*fa)
    do
      # borrowed some logic from https://www.biostars.org/p/294920/ to smoosh together sequences
      # otherwise, art with generate nreads PER Sequence, not per genome
      name=$(basename $fasta | sed "s|.fa||g")
      cat $fasta | grep -v "^>" |  awk  -v header=">${{name}}_concatenated" 'BEGIN {{ ORS=""; print header "\\n" }} {{ print }}' > {output.spikedir}/${{name}}.fasta
    done
    for fasta in D6331.refseq/genomes/*fasta
    do
      name=$(basename $fasta | sed "s|.fasta||g")
      cat $fasta | grep -v "^>" |  awk  -v header=">${{name}}_concatenated" 'BEGIN {{ ORS=""; print header "\\n" }} {{ print }}' > {output.mockdir}/${{name}}.fasta
    done
    rm -r tmp_spike

    """

rule make_input_table:
    """ this could be modified to incorporate different depth schemes per bug,
    but for a first pass we do even depths
    """
    input:
        mockdir="mock",
        spikedir="spikes",
    output:
        outtable="sim_tables/{rep}_depth{depth}_spike{frac}.tab"
    run:
        import glob
        import math
        mocks = glob.glob(input.mockdir + "/*.fasta")
        spikes= glob.glob(input.spikedir + "/*.fasta")
        depth = int(wildcards.depth)
        nmocks = len(mocks)
        nspikes = len(spikes)
        total_spike = depth * float(wildcards.frac)
        total_non_spike = depth - total_spike
        each_non_spike = math.floor(total_non_spike/nmocks)
        each_spike =  math.floor(total_spike/nspikes)
        with open(output.outtable, "w") as outf:
            for spike in spikes:
                outf.write(f"{spike}\t{each_spike}\n")
            for mock in mocks:
                outf.write(f"{mock}\t{each_non_spike}\n")


rule simulate:
    input:
        mockdir="mock",
        spikedir="spikes",
        intable="sim_tables/{rep}_depth{depth}_spike{frac}.tab"
    output:
        R1="{rep}_depth{depth}_spike{frac}_R1.fastq.gz",
        R2="{rep}_depth{depth}_spike{frac}_R2.fastq.gz",
        intermediate_trigger=touch("{rep}_depth{depth}_spike{frac}_individual_reference_fastq_present"),
    params:
        tmppre = lambda wildcards: f"tmp_{wildcards.rep}_depth{wildcards.depth}_spike{wildcards.frac}"
    container:"docker://ghcr.io/vdblab/art:2016.06.05"
    resources:
        runtime=4*60
    threads: 4
    shell: """
    set -euxo pipefail

    cat {input.intable} |  awk  '$2 != 0'  | while read ref nreads
    do
        refbase=$(basename $ref)
        art_illumina --seqSys HS25 --in $ref  --paired --len 150 --rcount $nreads --mflen 500 --sdev 10 --out {params.tmppre}_${{refbase}}_R --rndSeed {wildcards.rep}
    done

    ls {params.tmppre}*
    cat {params.tmppre}*R1.fq | pigz > {output.R1}
    cat {params.tmppre}*R2.fq | pigz > {output.R2}
    """

rule simulate_even_coverage_datasets:
    """The expand() in the intable arg is to just select the first table; we
    are just using it for a list of references needed for combining into our
    test data
    """
    input:
        mockdir="mock",
        spikedir="spikes",
        intable=expand("sim_tables/{rep}_depth{depth}_spike{frac}.tab", rep=reps[0], depth=total_depths[0], frac=total_spike_fractions[0])
    output:
        R1='{rep}_evencoverage{fcov}_R1.fastq.gz',
        R2='{rep}_evencoverage{fcov}_R2.fastq.gz',
        intermediate_trigger=touch("{rep}_evencoverage{fcov}_individual_reference_fastq_present"),
    params:
        tmppre = lambda wildcards: f"tmp_{wildcards.rep}_evencoverage{wildcards.fcov}"
    container:"docker://ghcr.io/vdblab/art:2016.06.05"
    threads: 4
    shell: """
    set -euxo pipefail
    # here we don't remove the 0 reads lines becuase those are only
    # relevant for the even-reads simulation
    cat {input.intable} | while read ref nreads
    do
      refbase=$(basename $ref)
      refdir=$(dirname $ref)
      echo $refdir
      if [ "$refdir" == "spikes" ]
      then
        echo "simulating fractional coverage for $refbase"
        art_illumina --seqSys HS25 --in $ref  --paired --len 150 --fcov {wildcards.fcov} --mflen 500 --sdev 10 --out {params.tmppre}_${{refbase}}_R --rndSeed {wildcards.rep}
      else
        echo "simulating 1x coverage for $refbase"
        art_illumina --seqSys HS25 --in $ref  --paired --len 150 --fcov 1 --mflen 500 --sdev 10 --out {params.tmppre}_${{refbase}}_R --rndSeed {wildcards.rep}
      fi

    done

    ls {params.tmppre}*
    cat {params.tmppre}*R1.fq | pigz > {output.R1}
    cat {params.tmppre}*R2.fq | pigz > {output.R2}
    """

rule stats:
    input:
        R1="{rep}_depth{depth}_spike{frac}_R1.fastq.gz",
        R2="{rep}_depth{depth}_spike{frac}_R2.fastq.gz",
        intermediate_trigger="{rep}_depth{depth}_spike{frac}_individual_reference_fastq_present",
    output:
        stats="{rep}_depth{depth}_spike{frac}.statsfastq"
    params:
        tmppre = lambda wildcards: f"tmp_{wildcards.rep}_depth{wildcards.depth}_spike{wildcards.frac}"
    container: "docker://ghcr.io/vdblab/seqkit:2.3.1"
    threads: 8
    shell:"""
    # gather all the per-reference fastq stats
    seqkit stats {params.tmppre}*.fq > {output.stats}
    # gather the final stats
    seqkit stats {input.R1} {input.R2} | tail -n+2  >> {output.stats}
    """


rule evenstats:
    input:
        R1="{rep}_evencoverage{fcov}_R1.fastq.gz",
        R2="{rep}_evencoverage{fcov}_R2.fastq.gz",
        intermediate_trigger="{rep}_evencoverage{fcov}_individual_reference_fastq_present",
    output:
        stats="{rep}_evencoverage{fcov}.statsfastq"
    params:
        tmppre = lambda wildcards: f"tmp_{wildcards.rep}_evencoverage{wildcards.fcov}"
    container: "docker://ghcr.io/vdblab/seqkit:2.3.1"
    threads: 8
    shell:"""
    # gather all the per-reference fastq stats
    seqkit stats {params.tmppre}*.fq > {output.stats}
    # gather the final stats
    seqkit stats {input.R1} {input.R2} | tail -n+2  >> {output.stats}
    """


rule spades:
    input:
        R1="{rep}_depth{depth}_spike{frac}_R1.fastq.gz",
        R2="{rep}_depth{depth}_spike{frac}_R2.fastq.gz",
    output:
        assembly="spades_{rep}_depth{depth}_spike{frac}.assembly.fasta",
        graph="spades_{rep}_depth{depth}_spike{frac}.assembly_graph.gfa",
        outdir=directory("spades_{rep}_depth{depth}_spike{frac}/"),
        spades_log="spades_{rep}_depth{depth}_spike{frac}/spades.log",
    container:
        config["docker_spades"]
    resources:
        mem_mb=58*1024,
        runtime=48 * 60,
    threads: 30
    log:
        e="logs/spades_{rep}_depth{depth}_spike{frac}.log",
    shell:
        """
        spades.py \
            -1 {input.R1} \
            -2 {input.R2} \
            -t {threads} \
            --meta \
            -o {output.outdir}/ \
            -m $(({resources.mem_mb}/1024)) \
            2> {log.e}
        mv {output.outdir}/scaffolds.fasta {output.assembly}
        mv {output.outdir}/assembly_graph_with_scaffolds.gfa {output.graph}
        """

use rule spades as spades_evencoverage with :
    # TODO: add in params for read length to experiment with larger kmers than default
    input:
        R1="{rep}_evencoverage{fcov}_R1.fastq.gz",
        R2="{rep}_evencoverage{fcov}_R2.fastq.gz",
    output:
        assembly="spades_{rep}_evencoverage{fcov}.assembly.fasta",
        graph="spades_{rep}_evencoverage{fcov}.assembly_graph.gfa",
        outdir=directory("spades_{rep}_evencoverage{fcov}/"),
        spades_log="spades_{rep}_evencoverage{fcov}/spades.log",
    log:
        e="logs/spades_{rep}_evencoverage{fcov}.log",


rule unzip_rename_fastq_for_metawrap:
    """ metawrap needs uncompressed fastqs with a specific naming convention
    otherwise, "Unable to find proper fastq read pair in the format *_1.fastq and *_2.fastq"
    """
    input:
        R1="{rep}_depth{depth}_spike{frac}_R1.fastq.gz",
        R2="{rep}_depth{depth}_spike{frac}_R2.fastq.gz",
    output:
        R1=temp("bintmp_{rep}_depth{depth}_spike{frac}_1.fastq"),
        R2=temp("bintmp_{rep}_depth{depth}_spike{frac}_2.fastq"),
    shell:
        """
        zcat {input.R1} > {output.R1}
        zcat {input.R2} > {output.R2}
        """
use rule unzip_rename_fastq_for_metawrap as unzip_rename_fastq_for_metawrap_evencoverage with :
    input:
        R1="{rep}_evencoverage{fcov}_R1.fastq.gz",
        R2="{rep}_evencoverage{fcov}_R2.fastq.gz",
    output:
        R1=temp("bintmp_{rep}_evencoverage{fcov}_1.fastq"),
        R2=temp("bintmp_{rep}_evencoverage{fcov}_2.fastq"),


rule metawrap_binning:
    """
    """
    input:
        R1="bintmp_{rep}_depth{depth}_spike{frac}_1.fastq",
        R2="bintmp_{rep}_depth{depth}_spike{frac}_2.fastq",
        assembly="spades_{rep}_depth{depth}_spike{frac}.assembly.fasta",
    output:
        stats="metawrap/rawbinning_{rep}_depth{depth}_spike{frac}/{tool}/{tool}_bins/{tool}.done",
    params:
        outdir=lambda wc, output: os.path.dirname(os.path.dirname(output.stats)),
    container:
        config["docker_metawrap"]
    threads: 48
    resources:
        mem_mb=32 * 1024,
        runtime=12 * 60,
    shell:
        """
        metawrap binning -o {params.outdir} -t {threads} -a {input.assembly} --{wildcards.tool} {input.R1} {input.R2}
        touch {output.stats}
        """

use rule metawrap_binning as metawrap_binning_evencoverage with:
    input:
        R1="bintmp_{rep}_evencoverage{fcov}_1.fastq",
        R2="bintmp_{rep}_evencoverage{fcov}_2.fastq",
        assembly="spades_{rep}_evencoverage{fcov}.assembly.fasta",
    output:
        stats="metawrap/rawbinning_{rep}_evencoverage{fcov}/{tool}/{tool}_bins/{tool}.done",

rule metawrap_refine_binning:
    """The names for the params binput_dirs is due to the crazy naming of the output of metawrap.
    The only consistant file we can use as a trigger is the <tool>.done file, but the refine module needs the dir beneath it under a path like
    metawrap/rawbinning_473/concoct/concoct_bins/concoct_bins
    """
    input:
        binputs=expand("metawrap/rawbinning_{{rep}}_depth{{depth}}_spike{{frac}}/{tool}/{tool}_bins/{tool}.done", tool=BINNING_TOOLS),
    output:
        stats=f'metawrap/refined_binning_{{rep}}_depth{{depth}}_spike{{frac}}/metawrap_{config["metawrap_compl_thresh"]}_{config["metawrap_contam_thresh"]}_bins.stats',
        stats_mqc=f'metawrap/refined_binning_{{rep}}_depth{{depth}}_spike{{frac}}/metawrap_{config["metawrap_compl_thresh"]}_{config["metawrap_contam_thresh"]}_bins.stats_mqc.tsv',
        contigs=f'metawrap/refined_binning_{{rep}}_depth{{depth}}_spike{{frac}}/metawrap_{config["metawrap_compl_thresh"]}_{config["metawrap_contam_thresh"]}_bins.contigs',
        bin1=f"metawrap/refined_binning_{{rep}}_depth{{depth}}_spike{{frac}}/{BINNING_TOOLS[0]}_bins.stats",
        bin2=f"metawrap/refined_binning_{{rep}}_depth{{depth}}_spike{{frac}}/{BINNING_TOOLS[1]}_bins.stats",
        bin3=f"metawrap/refined_binning_{{rep}}_depth{{depth}}_spike{{frac}}/{BINNING_TOOLS[2]}_bins.stats",
    params:
        outdir=lambda wc, output: os.path.dirname(output.stats),
        binput_dirs=lambda wc, input: [os.path.dirname(x) for x in input.binputs],
        completeness=config["metawrap_compl_thresh"],
        contamination=config["metawrap_contam_thresh"],
        checkm_db=config["checkm_db"],
    # this is a different container that has checkm installed
    container:
        config["docker_metawrap"]
    threads: 30
    # give it 82 gb memory because checkm estimates pplacer will need 40GB per core, and we want this to run reasonably fast
    resources:
        mem_mb=82 * 1024,
        runtime=12 * 60,
    shell:
        """
        export  CHECKM_DATA_PATH={params.checkm_db}
        metawrap bin_refinement -o {params.outdir} -t {threads} -A {params.binput_dirs[0]} -B {params.binput_dirs[1]} -C {params.binput_dirs[2]} -c {params.completeness} -x {params.contamination}
        echo -e "#id: 'metawrap'\n#plot_type: 'table'\n#section_name: 'Bin Refinement'" > {output.stats}_mqc.tsv && cat {output.stats} >> {output.stats}_mqc.tsv
        """

use rule metawrap_refine_binning as metawrap_refine_binning_evencoverage with:
    input:
        binputs=expand("metawrap/rawbinning_{{rep}}_evencoverage{{fcov}}/{tool}/{tool}_bins/{tool}.done", tool=BINNING_TOOLS),
    output:
        stats=f'metawrap/refined_binning_{{rep}}_evencoverage{{fcov}}/metawrap_{config["metawrap_compl_thresh"]}_{config["metawrap_contam_thresh"]}_bins.stats',
        stats_mqc=f'metawrap/refined_binning_{{rep}}_evencoverage{{fcov}}/metawrap_{config["metawrap_compl_thresh"]}_{config["metawrap_contam_thresh"]}_bins.stats_mqc.tsv',
        contigs=f'metawrap/refined_binning_{{rep}}_evencoverage{{fcov}}/metawrap_{config["metawrap_compl_thresh"]}_{config["metawrap_contam_thresh"]}_bins.contigs',
        bin1=f"metawrap/refined_binning_{{rep}}_evencoverage{{fcov}}/{BINNING_TOOLS[0]}_bins.stats",
        bin2=f"metawrap/refined_binning_{{rep}}_evencoverage{{fcov}}/{BINNING_TOOLS[1]}_bins.stats",
        bin3=f"metawrap/refined_binning_{{rep}}_evencoverage{{fcov}}/{BINNING_TOOLS[2]}_bins.stats",

rule cleanup:
    input:
        "{rep}_depth{depth}_spike{frac}_individual_reference_fastq_present",
        binstats,
    output:
        touch("clean_{rep}_depth{depth}_spike{frac}.done")
    params:
        tmppre = lambda wildcards: f"tmp_{wildcards.rep}_depth{wildcards.depth}_spike{wildcards.frac}"
    shell:
        """
        rm {params.tmppre}*fq
        rm {params.tmppre}*aln
        """
