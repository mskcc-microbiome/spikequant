# spikequant
Remove and Quantify Organism Spike Reads from Sequencing Data

## Prerequisites

Must have snakemake available in `PATH`.

## quick start
```
snakemake --directory testresults --snakefile workflow/Snakefile --config spiketable=$PWD/benchmarking/manifests/raw.csv
```

### dependencies
Either execute snakemake with `--use-singularity` (or the updated syntax for snakemake 8), a snakemake profile with singularity configured, or from a conda environment with the following dependencies precent:

- pigz
- ART read simulator
- spades
- metawrap
- seqkit


## Structure
Repo has been structured according to Snakemake's best practices. There are two supplementary workflows outside of this structure, to keep them from being imported when using tools like snakedeploy.  these are a benchmarking workflow and a test data generating workflow



## Test Data
Test data can be generated by running the following:

```
snakemake --snakefile testdata/Snakefile --directory $PWD/testdata/data/
```

configure the types of data it creates via the config file in testdata/config.yaml.  We have commited a 100k read dataset to the repo, along with its assembly and binning results. Due to the poor quality of this low-depth assembly, we don't bother running the bin refinement step of metawrap.

## Run `spikequant` on the testdata

```
snakemake --directory testresults

```
## Benchmarking
The benchmarking workflow compares alignment methods and off-target database effectiveness for spike remove
Benchmarking can be run on the Test data in the example config running the following, noting that `--directory` must be a location one file level deeper than the root for the relative paths to work.

```
snakemake --snakefile benchmarking/Snakefile --directory $PWD/benchmarking_results/
```

Note that the resulting coverages are wonky for the small test dataset because fastani's threshold of 3000bp excludes most of the assembly or bins, which effectively bypasses the dereplication.
